{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 复杂网络与社交网络分析课程设计"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 题目：K个影响力重要节点发现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Degree、PageRank、HITS、K-cores算法以及改进算法在数据集上的结果分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from operator import itemgetter\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import math\n",
    "import copy\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取 edgelist 文件并创建图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_graph_from_edgelist(file_path):\n",
    "    # 读文件，制图\n",
    "    f = open(file_path, 'r')\n",
    "    # 建立有向图\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    lines = f.readlines()\n",
    "    # 一次性读入整个文件，加快处理速度\n",
    "    for line in lines:\n",
    "        line = line.strip()     # 去掉结尾换行符\n",
    "        line = line.split()     # 以空格或回车分割句子\n",
    "        G.add_edge(int(line[0]), int(line[1]))\n",
    "        # 建立边关系\n",
    "    f.close()\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 重新加载已经存入txt的图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_graph_from_file(file_path):\n",
    "    G = nx.read_edgelist(file_path,create_using=nx.DiGraph())\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 绘制度分布图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_degree_plot(myGraph):\n",
    "    # 获取图的度分布直方图\n",
    "    Degree_Hist = nx.degree_histogram(myGraph)\n",
    "    # 初始化一个列表，用于存储处理后的度分布数据\n",
    "    Degree_Hist = [Degree_Hist, [], []]\n",
    "    # 遍历度分布直方图，处理为 (度数, 频数) 的形式\n",
    "    for i in range(0, len(Degree_Hist[0])):\n",
    "        if Degree_Hist[0][i] == 0:\n",
    "            continue\n",
    "        Degree_Hist[1].append(i)    # 存储度数\n",
    "        Degree_Hist[2].append(Degree_Hist[0][i])    # 存储对应的频数\n",
    "     # 设置中文显示和字体\n",
    "    plt.rcParams[\"font.sans-serif\"] = \"SimHei\"\n",
    "    plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "    # 创建一个图形，设置图形大小\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "     # 在图形中创建两个子图，分别绘制原始度数和对数化后的度数\n",
    "    axes1 = plt.subplot(121)\n",
    "    axes1.scatter([i for i in Degree_Hist[1]], [math.log(i + 1) for i in Degree_Hist[2]],\n",
    "                  s=0.5, ) # 绘制原始度数的散点图\n",
    "    axes1.set_xlabel(\"Degree\")\n",
    "    axes1.set_ylabel(\"log(count+1)\")\n",
    "    axes2 = plt.subplot(122)\n",
    "    axes2.scatter([math.log(i) for i in Degree_Hist[1]], [math.log(i + 1) for i in Degree_Hist[2]],\n",
    "                  s=0.8, )  # 绘制对数化后的度数的散点\n",
    "    axes2.set_xlabel(\"log(Degree)\")\n",
    "    axes2.set_ylabel(\"log(count+1)\")\n",
    "    plt.suptitle(\"度分布散点图\")\n",
    "    plt.savefig(\"./pic/度分布散点图.png\") # 保存图形到文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 清洗节点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_node(data_file_name, G: nx.DiGraph):\n",
    "    print('=' * 15 + f\"数据清洗\" + '=' * 15)\n",
    "    f1 = './data/{}.txt'.format(data_file_name)\n",
    "    f_clean = open(f1, 'w', encoding='utf-8')\n",
    "    f2 = './data/higgs-{}_network.edgelist'.format(data_file_name)\n",
    "    with open(f2, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            l = line.split(' ')\n",
    "            n1, n2 = int(l[0]), int(l[1])\n",
    "            if n1 in G.nodes and n2 in G.nodes:\n",
    "                f_clean.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 统计边列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#统计边列表,num表示图的节点数目,防止过大不方便绘制\n",
    "def stat_node(data_file_name, num):\n",
    "    f = './data/{}.txt'.format(data_file_name)\n",
    "    edgeList = []\n",
    "    count = 0\n",
    "    with open(f, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if count >= num:\n",
    "                return edgeList\n",
    "            edge = [int(one) for one in line.strip('\\n').split(' ')]\n",
    "            edgeList.append(tuple(edge))\n",
    "            count+=1\n",
    "    return edgeList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LT模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "__all__ = ['linear_threshold']\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "#  Some Famous Diffusion Models\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def linear_threshold(G, seeds, steps=0):           # LT线性阈值算法\n",
    "    \"\"\"\n",
    "      Parameters\n",
    "      ----------\n",
    "      G : networkx graph                     #所有节点构成的图\n",
    "          The number of nodes.\n",
    "      seeds: list of nodes                   #种子节点集\n",
    "          The seed nodes of the graph\n",
    "      steps: int                             #激活节点的层数（深度），当steps<=0时，返回子节点集能激活的所有节点\n",
    "          The number of steps to diffuse\n",
    "          When steps <= 0, the model diffuses until no more nodes\n",
    "          can be activated\n",
    "      Return\n",
    "      ------\n",
    "      layer_i_nodes : list of list of activated nodes\n",
    "        layer_i_nodes[0]: the seeds                  #子节点集\n",
    "        layer_i_nodes[k]: the nodes activated at the kth diffusion step   #该子节点集激活的节点集\n",
    "      Notes\n",
    "      -----\n",
    "      1. Each node is supposed to have an attribute \"threshold\".  If not, the\n",
    "         default value is given (0.5).    #每个节点有一个阈值，这里默认阈值为：0.5\n",
    "      2. Each edge is supposed to have an attribute \"influence\".  If not, the\n",
    "         default value is given (1/in_degree)  #每个边有一个权重值，这里默认为：1/入度\n",
    "      References\n",
    "      ----------\n",
    "      [1] GranovetterMark. Threshold models of collective behavior.\n",
    "          The American journal of sociology, 1978.\n",
    "    \"\"\"\n",
    "\n",
    "    if type(G) == nx.MultiGraph or type(G) == nx.MultiDiGraph:\n",
    "        raise Exception(\"linear_threshold() is not defined for graphs with multiedges.\")\n",
    "\n",
    "    # make sure the seeds are in the graph\n",
    "    for s in seeds:\n",
    "        if s not in G.nodes():\n",
    "            raise Exception(\"seed\", s, \"is not in graph\")\n",
    "\n",
    "    # change to directed graph\n",
    "    if not G.is_directed():\n",
    "        DG = G.to_directed()\n",
    "    else:\n",
    "        DG = copy.deepcopy(G)        # copy.deepcopy 深拷贝 拷贝对象及其子对象\n",
    "\n",
    "    # init thresholds\n",
    "    for n in DG.nodes():\n",
    "        if 'threshold' not in DG.nodes[n]:\n",
    "            DG.nodes[n]['threshold'] = 0.5\n",
    "        elif DG.nodes[n]['threshold'] > 1:\n",
    "            raise Exception(\"node threshold:\", DG.nodes[n]['threshold'], \"cannot be larger than 1\")\n",
    "\n",
    "    #init influences\n",
    "    in_deg = DG.in_degree()       # 获取所有节点的入度\n",
    "    for e in DG.edges():\n",
    "        if 'influence' not in DG[e[0]][e[1]]:\n",
    "            DG[e[0]][e[1]]['influence'] = 1.0 / in_deg[e[1]]    # 计算边的权重\n",
    "        elif DG[e[0]][e[1]]['influence'] > 1:\n",
    "            raise Exception(\"edge influence:\", DG[e[0]][e[1]]['influence'], \"cannot be larger than 1\")\n",
    "\n",
    "    # perform diffusion\n",
    "    A = copy.deepcopy(seeds)\n",
    "    if steps <= 0:\n",
    "        # perform diffusion until no more nodes can be activated\n",
    "        return _diffuse_all(DG, A)\n",
    "    # perform diffusion for at most \"steps\" rounds only\n",
    "    return _diffuse_k_rounds(DG, A, steps)\n",
    "\n",
    "\n",
    "def _diffuse_all(G, A):\n",
    "    layer_i_nodes = []\n",
    "    layer_i_nodes.append([i for i in A])\n",
    "    while True:\n",
    "        len_old = len(A)\n",
    "        A, activated_nodes_of_this_round = _diffuse_one_round(G, A)\n",
    "        layer_i_nodes.append(activated_nodes_of_this_round)\n",
    "        if len(A) == len_old:\n",
    "            break\n",
    "    return layer_i_nodes\n",
    "\n",
    "\n",
    "def _diffuse_k_rounds(G, A, steps):\n",
    "    layer_i_nodes = []\n",
    "    layer_i_nodes.append([i for i in A])\n",
    "    while steps > 0 and len(A) < len(G):\n",
    "        len_old = len(A)\n",
    "        A, activated_nodes_of_this_round = _diffuse_one_round(G, A)\n",
    "        layer_i_nodes.append(activated_nodes_of_this_round)\n",
    "        if len(A) == len_old:\n",
    "            break\n",
    "        steps -= 1\n",
    "    return layer_i_nodes\n",
    "\n",
    "\n",
    "def _diffuse_one_round(G, A):\n",
    "    activated_nodes_of_this_round = set()\n",
    "    for s in A:\n",
    "        nbs = G.successors(s)\n",
    "        for nb in nbs:\n",
    "            if nb in A:\n",
    "                continue\n",
    "            active_nb = list(set(G.predecessors(nb)).intersection(set(A)))\n",
    "            if _influence_sum(G, active_nb, nb) >= G.nodes[nb]['threshold']:\n",
    "                activated_nodes_of_this_round.add(nb)\n",
    "    A.extend(list(activated_nodes_of_this_round))\n",
    "    return A, list(activated_nodes_of_this_round)\n",
    "\n",
    "\n",
    "def _influence_sum(G, froms, to):\n",
    "    influence_sum = 0.0\n",
    "    for f in froms:\n",
    "        influence_sum += G[f][to]['influence']\n",
    "    return influence_sum\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#PageRank\n",
    "def calculate_pagerank(graph):\n",
    "    # 计算 PageRank 值\n",
    "    #pagerank_scores = nx.pagerank(graph)\n",
    "    pagerank_scores = func_pagerank(graph)\n",
    "    return pagerank_scores\n",
    "\n",
    "# 获取 PageRank 值最高的 k 个节点\n",
    "def top_influential_nodes(scores):\n",
    "    top_nodes = sorted(scores, key=scores.get, reverse=True)\n",
    "    return top_nodes\n",
    "\n",
    "def func_pagerank(G):\n",
    "    alpha=0.85\n",
    "    personalization=None\n",
    "    max_iter=100\n",
    "    tol=1.0e-6\n",
    "    nstart=None\n",
    "    weight=\"weight\"\n",
    "    dangling=None\n",
    "\n",
    "    N = len(G)\n",
    "    if N == 0:\n",
    "        return {}\n",
    "    nodelist = list(G)\n",
    "    A = nx.to_scipy_sparse_array(G, nodelist=nodelist, weight=weight, dtype=float)\n",
    "    S = A.sum(axis=1)\n",
    "    S[S != 0] = 1.0 / S[S != 0]\n",
    "\n",
    "    Q = sp.sparse.csr_array(sp.sparse.spdiags(S.T, 0, *A.shape))\n",
    "    A = Q @ A\n",
    "    # 初始向量\n",
    "    if nstart is None:\n",
    "        x = np.repeat(1.0 / N, N)\n",
    "    else:\n",
    "        x = np.array([nstart.get(n, 0) for n in nodelist], dtype=float)\n",
    "        x /= x.sum()\n",
    "\n",
    "    # 个性化向量\n",
    "    if personalization is None:\n",
    "        p = np.repeat(1.0 / N, N)\n",
    "    else:\n",
    "        p = np.array([personalization.get(n, 0) for n in nodelist], dtype=float)\n",
    "        if p.sum() == 0:\n",
    "            raise ZeroDivisionError\n",
    "        p /= p.sum()\n",
    "     # 悬空节点\n",
    "    if dangling is None:\n",
    "        dangling_weights = p\n",
    "    else:\n",
    "         # 将悬空节点的字典转换为按nodelist顺序的数组\n",
    "        dangling_weights = np.array([dangling.get(n, 0) for n in nodelist], dtype=float)\n",
    "        dangling_weights /= dangling_weights.sum()\n",
    "    is_dangling = np.where(S == 0)[0]\n",
    "\n",
    "    # 幂迭代：进行最多max_iter次迭代\n",
    "    for _ in range(max_iter):\n",
    "        xlast = x\n",
    "        x = alpha * (x @ A + sum(x[is_dangling]) * dangling_weights) + (1 - alpha) * p\n",
    "         # 检查收敛性，l1范数\n",
    "        err = np.absolute(x - xlast).sum()\n",
    "        if err < N * tol:\n",
    "            return dict(zip(nodelist, map(float, x)))\n",
    "    raise nx.PowerIterationFailedConvergence(max_iter)\n",
    "\n",
    "#打印前100个得分最高的节点\n",
    "def my_PageRank(graph):\n",
    "     # 计算 PageRank 值\n",
    "    pagerank_scores = calculate_pagerank(graph)\n",
    "    # 获取 PageRank 值最高的 k 个节点\n",
    "    top_nodes = top_influential_nodes(pagerank_scores)\n",
    "\n",
    "    # # 设置要获取的影响力最大节点数\n",
    "    # top_nodes_k = top_nodes[:100]\n",
    "    # # 打印结果Top {k} Influential Nodes\n",
    "    # f = open(\"PageRank.txt\", 'w')\n",
    "    # for node in top_nodes_k:\n",
    "    #     f.write(f\"Node:{node}  ,  PageRank Score:{pagerank_scores[node]}\\n\")\n",
    "\n",
    "    return top_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HITS模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#HITS模块\n",
    "\n",
    "def calculate_hits(graph):\n",
    "    # 计算 HITS 算法得到 Hub 和 Authority 值\n",
    "    #hits_scores = nx.hits(graph)\n",
    "    hits_scores = func_HITS(graph)\n",
    "    return hits_scores\n",
    "\n",
    "def func_HITS(G):\n",
    "    max_iter=100\n",
    "    tol=1.0e-8\n",
    "    nstart=None\n",
    "    normalized=True\n",
    "    if len(G) == 0:\n",
    "        return {}, {}\n",
    "    A = nx.adjacency_matrix(G, nodelist=list(G), dtype=float)\n",
    "\n",
    "    if nstart is None:\n",
    "        _, _, vt = sp.sparse.linalg.svds(A, k=1, maxiter=max_iter, tol=tol)\n",
    "    else:\n",
    "        nstart = np.array(list(nstart.values()))\n",
    "        _, _, vt = sp.sparse.linalg.svds(A, k=1, v0=nstart, maxiter=max_iter, tol=tol)\n",
    "\n",
    "    a = vt.flatten().real\n",
    "    h = A @ a\n",
    "    if normalized:\n",
    "        h /= h.sum()\n",
    "        a /= a.sum()\n",
    "    hubs = dict(zip(G, map(float, h)))\n",
    "    authorities = dict(zip(G, map(float, a)))\n",
    "    return hubs, authorities\n",
    "\n",
    "def top_influential_nodes(scores):\n",
    "    # 获取 Hub 或 Authority 值最高的 k 个节点\n",
    "    top_nodes = sorted(scores, key=scores.get, reverse=True)\n",
    "    return top_nodes\n",
    "\n",
    "def my_HITS(graph):\n",
    "    # 计算 HITS 算法得到 Hub 和 Authority 值\n",
    "    hits_scores = calculate_hits(graph)\n",
    "    # 获取 Authority 值最高的 k 个节点\n",
    "    authority_nodes = top_influential_nodes(hits_scores[1])\n",
    "\n",
    "    # 打印结果Top {k} Influential Nodes based on Authority:\n",
    "    # top_authority_nodes = authority_nodes[:100]\n",
    "    # f = open(\"HITS.txt\", 'w')\n",
    "    # for node in top_authority_nodes:\n",
    "    #     f.write(f\"Node:{node}  ,  Authority Score:{hits_scores[1][node]}\\n\")\n",
    "\n",
    "    return authority_nodes\n",
    "\n",
    "def get_HITS_Nodes(G):\n",
    "    h,a=func_HITS(G)\n",
    "    h1 = sorted(h.items(), key=itemgetter(1), reverse=True)  # 排序\n",
    "    a1 = sorted(a.items(), key=itemgetter(1), reverse=True)  # 排序\n",
    "    hhr = [e[0] for e in h1]   # 取节点\n",
    "    har = [e[0] for e in a1]   # 取节点\n",
    "    return hhr,har"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kcores模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from operator import itemgetter\n",
    "\n",
    "def K_cores(G, G2):\n",
    "    # 通过各个节点的核数发现重要节点\n",
    "    core = 1\n",
    "    flag = 1\n",
    "    node_cores = {}\n",
    "\n",
    "    for node in G.nodes():\n",
    "        node_cores[node] = -1\n",
    "\n",
    "    while(G.number_of_nodes() != 0):\n",
    "        # 循环去除\n",
    "        while(flag == 1):\n",
    "            # 还有变化\n",
    "            flag = 0\n",
    "            nodes = list(G.nodes())\n",
    "            for node in nodes:\n",
    "                if G.degree(node) <= core:\n",
    "                    node_cores[node] = core\n",
    "                    flag = 1\n",
    "                    G.remove_node(node)\n",
    "        flag = 1\n",
    "        core += 1\n",
    "\n",
    "    # print(node_cores)\n",
    "    cores1 = sorted(node_cores.items(), key=itemgetter(1), reverse=True)  # 排序\n",
    "\n",
    "    # ncores = cores1[:100]\n",
    "    # f = open(\"K-coresRank.txt\", 'w')\n",
    "    # f.write(\"重要节点排序:\\n\" + str(ncores)+\"\\n\\n各节点的K-coresRank值:\\n\" + str(cores1))\n",
    "    # f.close()\n",
    "\n",
    "    return cores1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 度中心性模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#度中心性模块\n",
    "\n",
    "def degree_centrality(G):\n",
    "    # 度中心性\n",
    "    dc = nx.degree_centrality(G)  # degree\n",
    "    dc1 = sorted(dc.items(), key=itemgetter(1), reverse=True)  # 排序\n",
    "\n",
    "    # ndc = [e[0] for e in dc1[:100]]   # 取节点\n",
    "    # f = open(\"DegreeRank.txt\", 'w')\n",
    "    # f.write(\"重要节点排序:\\n\" + str(ndc) + \"\\n\\n各节点的度数中心性:\\n\" + str(dc1))\n",
    "    # f.close()\n",
    "\n",
    "    return dc1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 卷积改进方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv(G,atr):#atr是计算出来的degree\n",
    "    conv={}#建立字典\n",
    "    for edge in G.edges():\n",
    "        if edge[0] in conv.keys():\n",
    "            if edge[1] in atr.keys():\n",
    "                conv[edge[0]] += atr[edge[1]] / atr[edge[0]]\n",
    "        else:\n",
    "            if edge[1] in atr.keys():\n",
    "                conv[edge[0]] = atr[edge[1]] / atr[edge[0]]\n",
    "            else:\n",
    "                conv[edge[0]]=0\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算单位时间内top-k节点的影响力范围"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 通过LT线性阈值模型，计算单位时间内top-k节点的影响力范围\n",
    "def cal_influence(G, seeds, k=1): \n",
    "    layers = linear_threshold(G, seeds, k)      # seeds为种子节点集合\n",
    "    length = 0\n",
    "    for i in range(1, len(layers)):\n",
    "        length += len(layers[i])        # 计算每个种子节点激活节点的个数之和\n",
    "    return length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sort(dic):\n",
    "    dic1 = sorted(dic.items(), key=itemgetter(1), reverse=True)  # 排序\n",
    "    dnode = [e[0] for e in dic1]   # 取节点\n",
    "    return dnode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Primal_Info(myGraph: nx.Graph,fn):\n",
    "    # 输出图的基本结构\n",
    "    f = open(\"attribute_\" + fn + \".txt\", 'w')\n",
    "    f.write(\"节点数: \" + str(myGraph.number_of_nodes()))\n",
    "    f.write(\"\\n\\n连边数: \" + str(myGraph.number_of_edges()))\n",
    "    f.write(\"\\n\\n网络密度: \" + str(nx.density(myGraph)))\n",
    "    f.write(\"\\n\\n平均网络的聚类系数: \" + str(nx.average_clustering(nx.DiGraph(myGraph))))\n",
    "    f.write(\"\\n\\n整个网络的聚类系数: \" + str(nx.transitivity(myGraph)))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#主函数\n",
    "myGraph = nx.read_edgelist(\"./data/higgs-social_network.edgelist\",create_using=nx.DiGraph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compare_all_methods比较所有方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#compare_all_methods比较所有方法\n",
    "atr=dict(myGraph.out_degree())\n",
    "\n",
    "conv1=conv(myGraph,atr)\n",
    "convself=dict()\n",
    "convself2=dict()\n",
    "convself3=dict()\n",
    "for k in atr.keys():\n",
    "    if k in conv1.keys():\n",
    "        convself2[k] = atr[k]+0.5*conv1[k]\n",
    "        convself[k] = atr[k] + 0.2*conv1[k]\n",
    "        convself3[k] = atr[k]+conv1[k]\n",
    "    else:\n",
    "        convself2[k] = atr[k]\n",
    "        convself[k] = atr[k]\n",
    "        convself3[k] = atr[k]\n",
    "# 输出图的基本结构\n",
    "k = 10\n",
    "x = []\n",
    "y1 = []\n",
    "y2 = []\n",
    "y3 = []\n",
    "y4=[]\n",
    "y5=[]\n",
    "y6=[]\n",
    "y7=[]\n",
    "y8=[]\n",
    "\n",
    "PageRank_nodes=my_PageRank(myGraph)\n",
    "HITS_hubs,HITS_authorities = get_HITS_Nodes(myGraph)\n",
    "myGraph_copy = myGraph.copy()\n",
    "kcores = K_cores(myGraph_copy, myGraph)  \n",
    "degree_centality = degree_centrality(myGraph)\n",
    "\n",
    "nc1=sort(convself)\n",
    "nc2=sort(convself2)\n",
    "nc3=sort(convself3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " for seeds_num in range(200, 410,50):\n",
    "    seed_node = str(seeds_num)\n",
    "    if seed_node not in myGraph:\n",
    "        print(f\"Seed node {seed_node} is not in the graph.\")\n",
    "        continue \n",
    "    print('='*20+seed_node+\"\\n\")\n",
    "    x.append(seed_node)\n",
    "\n",
    "    #print(\"degree_centality[1]={}\\n\".format(degree_centality[1]))\n",
    "    degree_centality_seeds = [e[0] for e in degree_centality[:seeds_num]] \n",
    "\n",
    "    #print(\"kcores[1]={}\\n\".format(kcores[1]))\n",
    "    kcores_seeds = [k[0] for k in kcores[:seeds_num]] \n",
    "\n",
    "    #print(\"PageRank_nodes[1]={}\\n\".format(PageRank_nodes[1]))\n",
    "    PageRank_nodes_seeds = [p[0] for p in PageRank_nodes[:seeds_num]] \n",
    "\n",
    "    #print(\"HITS_hubs[1]={}\\n\".format(HITS_hubs[1]))\n",
    "    HITS_hubs_seeds = [h[0] for h in HITS_hubs[:seeds_num]] \n",
    "\n",
    "    #print(\"HITS_authorities[1]={}\\n\".format(HITS_authorities[1]))\n",
    "    HITS_authorities_seeds = [a[0] for a in HITS_authorities[:seeds_num]] \n",
    "\n",
    "    #print(\"nc1[1]={}\\n\".format(nc1[1]))\n",
    "    nc1_seeds = [n[0] for n in nc1[:seeds_num]] \n",
    "    nc2_seeds = [n[0] for n in nc2[:seeds_num]] \n",
    "    nc3_seeds = [n[0] for n in nc3[:seeds_num]] \n",
    "\n",
    "    y1.append(cal_influence(myGraph, degree_centality_seeds, k))\n",
    "    y2.append(cal_influence(myGraph, kcores_seeds, k))\n",
    "    y3.append(cal_influence(myGraph, nc1_seeds, k))\n",
    "    y4.append(cal_influence(myGraph, nc2_seeds, k))\n",
    "    y5.append(cal_influence(myGraph, PageRank_nodes_seeds, k))\n",
    "    y6.append(cal_influence(myGraph, HITS_hubs_seeds, k))\n",
    "    y7.append(cal_influence(myGraph, HITS_authorities_seeds, k))\n",
    "    y8.append(cal_influence(myGraph, nc3_seeds, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l1 = plt.plot(x, y1, 'r--', label='Degree')\n",
    "l2 = plt.plot(x, y2, 'c--', label='Kcores')\n",
    "l3 = plt.plot(x, y3, 'b--', label='Conv0.2')\n",
    "l4 = plt.plot(x, y4, 'k--', label='Conv0.5')\n",
    "l8 = plt.plot(x, y8, 'y--', label='Conv1')\n",
    "l5 = plt.plot(x, y5, 'g--', label='Pagerank')\n",
    "l6 = plt.plot(x, y6, 'm--', label='HITS-H')\n",
    "l7 = plt.plot(x, y7, 'm--', label='HITS-A')\n",
    "print(\"end\")\n",
    "plt.plot(x, y1, 'ro-', x, y2, 'c+-', x, y3, 'b^-', x, y4, 'ks-' , x, y5, 'gp-', x, y6, 'm1-', x, y7, 'm2-', x, y8, 'y3-')\n",
    "plt.legend()\n",
    "plt.savefig('Compare.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
